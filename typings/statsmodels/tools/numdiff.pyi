"""
This type stub file was generated by pyright.
"""

from statsmodels.compat.pandas import Appender, Substitution

"""numerical differentiation function, gradient, Jacobian, and Hessian

Author : josef-pkt
License : BSD

Notes
-----
These are simple forward differentiation, so that we have them available
without dependencies.

* Jacobian should be faster than numdifftools because it does not use loop over
  observations.
* numerical precision will vary and depend on the choice of stepsizes
"""
EPS = ...
_hessian_docs = ...
def approx_fprime(x, f, epsilon=..., args=..., kwargs=..., centered=...): # -> ndarray[tuple[int], dtype[Any]] | ndarray[_AnyShape, dtype[Any]]:
    '''
    Gradient of function, or Jacobian if function f returns 1d array

    Parameters
    ----------
    x : ndarray
        parameters at which the derivative is evaluated
    f : function
        `f(*((x,)+args), **kwargs)` returning either one value or 1d array
    epsilon : float, optional
        Stepsize, if None, optimal stepsize is used. This is EPS**(1/2)*x for
        `centered` == False and EPS**(1/3)*x for `centered` == True.
    args : tuple
        Tuple of additional arguments for function `f`.
    kwargs : dict
        Dictionary of additional keyword arguments for function `f`.
    centered : bool
        Whether central difference should be returned. If not, does forward
        differencing.

    Returns
    -------
    grad : ndarray
        gradient or Jacobian

    Notes
    -----
    If f returns a 1d array, it returns a Jacobian. If a 2d array is returned
    by f (e.g., with a value for each observation), it returns a 3d array
    with the Jacobian of each observation with shape xk x nobs x xk. I.e.,
    the Jacobian of the first observation would be [:, 0, :]
    '''
    ...

def approx_fprime_cs(x, f, epsilon=..., args=..., kwargs=...): # -> NDArray[Any]:
    '''
    Calculate gradient or Jacobian with complex step derivative approximation

    Parameters
    ----------
    x : ndarray
        parameters at which the derivative is evaluated
    f : function
        `f(*((x,)+args), **kwargs)` returning either one value or 1d array
    epsilon : float, optional
        Stepsize, if None, optimal stepsize is used. Optimal step-size is
        EPS*x. See note.
    args : tuple
        Tuple of additional arguments for function `f`.
    kwargs : dict
        Dictionary of additional keyword arguments for function `f`.

    Returns
    -------
    partials : ndarray
       array of partial derivatives, Gradient or Jacobian

    Notes
    -----
    The complex-step derivative has truncation error O(epsilon**2), so
    truncation error can be eliminated by choosing epsilon to be very small.
    The complex-step derivative avoids the problem of round-off error with
    small epsilon because there is no subtraction.
    '''
    ...

def approx_hess_cs(x, f, epsilon=..., args=..., kwargs=...): # -> NDArray[floating[Any]]:
    '''Calculate Hessian with complex-step derivative approximation

    Parameters
    ----------
    x : array_like
       value at which function derivative is evaluated
    f : function
       function of one array f(x)
    epsilon : float
       stepsize, if None, then stepsize is automatically chosen

    Returns
    -------
    hess : ndarray
       array of partial second derivatives, Hessian

    Notes
    -----
    based on equation 10 in
    M. S. RIDOUT: Statistical Applications of the Complex-step Method
    of Numerical Differentiation, University of Kent, Canterbury, Kent, U.K.

    The stepsize is the same for the complex and the finite difference part.
    '''
    ...

@Substitution(scale="3", extra_params="""return_grad : bool
        Whether or not to also return the gradient
""", extra_returns="""grad : nparray
        Gradient if return_grad == True
""", equation_number="7", equation="""1/(d_j*d_k) * ((f(x + d[j]*e[j] + d[k]*e[k]) - f(x + d[j]*e[j])))
""")
@Appender(_hessian_docs)
def approx_hess1(x, f, epsilon=..., args=..., kwargs=..., return_grad=...): # -> tuple[NDArray[floating[Any]], Any] | NDArray[floating[Any]]:
    ...

@Substitution(scale="3", extra_params="""return_grad : bool
        Whether or not to also return the gradient
""", extra_returns="""grad : ndarray
        Gradient if return_grad == True
""", equation_number="8", equation="""1/(2*d_j*d_k) * ((f(x + d[j]*e[j] + d[k]*e[k]) - f(x + d[j]*e[j])) -
                 (f(x + d[k]*e[k]) - f(x)) +
                 (f(x - d[j]*e[j] - d[k]*e[k]) - f(x + d[j]*e[j])) -
                 (f(x - d[k]*e[k]) - f(x)))
""")
@Appender(_hessian_docs)
def approx_hess2(x, f, epsilon=..., args=..., kwargs=..., return_grad=...): # -> tuple[NDArray[floating[Any]], Any] | NDArray[floating[Any]]:
    ...

@Substitution(scale="4", extra_params="", extra_returns="", equation_number="9", equation="""1/(4*d_j*d_k) * ((f(x + d[j]*e[j] + d[k]*e[k]) - f(x + d[j]*e[j]
                                                     - d[k]*e[k])) -
                 (f(x - d[j]*e[j] + d[k]*e[k]) - f(x - d[j]*e[j]
                                                     - d[k]*e[k]))""")
@Appender(_hessian_docs)
def approx_hess3(x, f, epsilon=..., args=..., kwargs=...): # -> NDArray[floating[Any]]:
    ...

approx_hess = ...
